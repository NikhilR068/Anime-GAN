{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import cv2\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.core import Flatten, Dropout\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "import scipy\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def get_generator(noise_shape):\n",
    "    noise_shape = noise_shape\n",
    "    \n",
    "    kernel_init = 'glorot_uniform'\n",
    "    \n",
    "    gen_input = Input(shape = noise_shape)\n",
    "    generator = Conv2DTranspose(filters = 512, kernel_size = (4,4), strides = (1,1), padding = \"valid\", data_format = \"channels_last\", kernel_initializer = kernel_init)(gen_input)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "        \n",
    "    generator = Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    generator = Conv2DTranspose(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    generator = Conv2DTranspose(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    generator = Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    generator = Conv2DTranspose(filters = 3, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = Activation('tanh')(generator)\n",
    "        \n",
    "    gen_opt = Adam(lr=0.00015, beta_1=0.5)\n",
    "    generator_model = Model(input = gen_input, output = generator)\n",
    "    generator_model.compile(loss='binary_crossentropy', optimizer=gen_opt, metrics=['accuracy'])\n",
    "    generator_model.summary()\n",
    "\n",
    "    return generator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(image_shape=(64,64,3)):\n",
    "    image_shape = image_shape\n",
    "    \n",
    "    dropout_prob = 0.4\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    dis_input = Input(shape = image_shape)\n",
    "    discriminator = Conv2D(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(dis_input)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    discriminator = Conv2D(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "   \n",
    "    discriminator = Conv2D(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    \n",
    "    discriminator = Conv2D(filters = 512, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    \n",
    "    discriminator = Flatten()(discriminator)\n",
    "    \n",
    "    discriminator = Dense(1)(discriminator)\n",
    "    discriminator = Activation('sigmoid')(discriminator)\n",
    "    \n",
    "    dis_opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    discriminator_model = Model(input = dis_input, output = discriminator)\n",
    "    discriminator_model.compile(loss='binary_crossentropy', optimizer=dis_opt, metrics=['accuracy'])\n",
    "    discriminator_model.summary()\n",
    "    return discriminator_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "# normalizing between 1 and -1\n",
    "def norm_img(img):\n",
    "    return (img / 127.5) - 1\n",
    "    \n",
    "# denormalizing for final output\n",
    "def denorm_img(img):\n",
    "    img = (img + 1) * 127.5\n",
    "    return img.astype(np.uint8) \n",
    "\n",
    "\n",
    "def sample_from_dataset(batch_size, image_shape, data_dir=None, data = None):\n",
    "    sample_dim = (batch_size,) + image_shape\n",
    "    sample = np.empty(sample_dim, dtype=np.float32)\n",
    "    all_data_dirlist = list(glob.glob(data_dir))\n",
    "    sample_imgs_paths = np.random.choice(all_data_dirlist,batch_size)\n",
    "    for index,img_filename in enumerate(sample_imgs_paths):\n",
    "        image = Image.open(img_filename)\n",
    "        image = image.resize(image_shape[:-1])\n",
    "        image = image.convert('RGB')\n",
    "        image = np.asarray(image)\n",
    "        image = norm_img(image)\n",
    "        sample[index,...] = image\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add normally distributed noise to generator\n",
    "def gen_noise(batch_size, shape):\n",
    "    return np.random.normal(0, 1, size=(batch_size,)+ shape)\n",
    "\n",
    "\n",
    "def generate_images(generator, save_dir):\n",
    "    \n",
    "    noise = gen_noise(batch_size,noise_shape)\n",
    "    fake_data_X = generator.predict(noise)\n",
    "    print(\"Generated Image show\")\n",
    "    plt.figure(figsize=(4,4))\n",
    "    gs1 = gridspec.GridSpec(4, 4)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "    rand_indices = np.random.choice(fake_data_X.shape[0],16,replace=False)\n",
    "    for i in range(16):\n",
    "        #plt.subplot(4, 4, i+1)\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        rand_index = rand_indices[i]\n",
    "        image = fake_data_X[rand_index, :,:,:]\n",
    "        fig = plt.imshow(denorm_img(image))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir+str(time.time())+\"_GENimage.png\",bbox_inches='tight',pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def save_img_batch(img_batch,img_save_dir):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    gs1 = gridspec.GridSpec(4, 4)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "    rand_indices = np.random.choice(img_batch.shape[0],16,replace=False)\n",
    "    for i in range(16):\n",
    "        \n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        rand_index = rand_indices[i]\n",
    "        image = img_batch[rand_index, :,:,:]\n",
    "        fig = plt.imshow(denorm_img(image))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_dir,bbox_inches='tight',pad_inches=0)\n",
    "    plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_shape = (1,1,100)\n",
    "num_steps = 10000\n",
    "batch_size = 64\n",
    "image_shape = None\n",
    "img_save_dir = \"Anime_GAN/Output\"\n",
    "save_model = True\n",
    "image_shape = (64,64,3)\n",
    "data_dir =  \"Anime_GAN\\\\animeface-character-dataset\\\\data\\\\*.png\"\n",
    "\n",
    "log_dir = img_save_dir\n",
    "save_model_dir = img_save_dir\n",
    "\n",
    "discriminator = get_discriminator(image_shape)\n",
    "generator = get_generator(noise_shape)\n",
    "discriminator.trainable = False\n",
    "\n",
    "opt = Adam(lr=0.00015, beta_1=0.5) \n",
    "gen_inp = Input(shape=noise_shape)\n",
    "GAN_inp = generator(gen_inp)\n",
    "GAN_opt = discriminator(GAN_inp)\n",
    "gan = Model(input = gen_inp, output = GAN_opt)\n",
    "gan.compile(loss = 'binary_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "gan.summary()\n",
    "\n",
    "avg_disc_fake_loss = deque([0], maxlen=250)     \n",
    "avg_disc_real_loss = deque([0], maxlen=250)\n",
    "avg_GAN_loss = deque([0], maxlen=250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for step in range(num_steps): \n",
    "    total_step = step\n",
    "    print(\"Begin step: \", total_step)\n",
    "    step_begin_time = time.time() \n",
    "    \n",
    "    real_data_X = sample_from_dataset(batch_size, image_shape, data_dir = data_dir)\n",
    "\n",
    "    noise = gen_noise(batch_size,noise_shape)\n",
    "    \n",
    "    fake_data_X = generator.predict(noise)\n",
    "    \n",
    "    if (tot_step % 10) == 0:\n",
    "        step_num = str(tot_step).zfill(4)\n",
    "        save_img_batch(fake_data_X,img_save_dir+step_num+\"_image.png\")\n",
    "\n",
    "        \n",
    "    data_X = np.concatenate([real_data_X,fake_data_X])\n",
    "    #add noise to labels\n",
    "    real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "    \n",
    "    \n",
    "    fake_data_Y = np.random.random_sample(batch_size)*0.2\n",
    "     \n",
    "    data_Y = np.concatenate((real_data_Y,fake_data_Y))\n",
    "        \n",
    "    discriminator.trainable = True\n",
    "    generator.trainable = False\n",
    "    \n",
    "    dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y)  \n",
    "    dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y)   \n",
    "    \n",
    "    print(\"Discriminator: fake loss: %f real loss : %f\" % (dis_metrics_fake[0],dis_metrics_real[0]))\n",
    "    \n",
    "    \n",
    "    avg_disc_fake_loss.append(dis_metrics_fake[0])\n",
    "    avg_disc_real_loss.append(dis_metrics_real[0])\n",
    "    \n",
    "    generator.trainable = True\n",
    "\n",
    "    GAN_X = gen_noise(batch_size,noise_shape)\n",
    "\n",
    "    GAN_Y = real_data_Y\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan_metrics = gan.train_on_batch(GAN_X,GAN_Y)\n",
    "    print(\"GAN loss: %f\" % (gan_metrics[0]))\n",
    "    \n",
    "    text_file = open(log_dir+\"\\\\training_log.txt\", \"a\")\n",
    "    text_file.write(\"Step: %d Disc: real loss: %f fake loss: %f GAN loss: %f\\n\" % (total_step, dis_metrics_real[0],\n",
    "                                                                                   dis_metrics_fake[0],gan_metrics[0]))\n",
    "    text_file.close()\n",
    "    avg_GAN_loss.append(gan_metrics[0])\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    if ((total_step+1) % 500) == 0:\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Average Disc_fake loss: %f\" % (np.mean(avg_disc_fake_loss)))    \n",
    "        print(\"Average Disc_real loss: %f\" % (np.mean(avg_disc_real_loss)))    \n",
    "        print(\"Average GAN loss: %f\" % (np.mean(avg_GAN_loss)))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        discriminator.trainable = True\n",
    "        generator.trainable = True\n",
    "        generator.save(save_model_dir+str(tot_step)+\"_GENERATOR_weights_and_arch.hdf5\")\n",
    "        discriminator.save(save_model_dir+str(tot_step)+\"_DISCRIMINATOR_weights_and_arch.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
